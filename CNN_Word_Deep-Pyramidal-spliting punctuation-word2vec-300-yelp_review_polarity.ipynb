{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='left' style=\"width:400px;height:120px;overflow:hidden;\">\n",
    "<img align='left' style='display: block;height: 92%' src='https://raw.githubusercontent.com/rio-group/machine-learning-course/master/imgs/riotext-small.png' alt='RIO logo' title='RIO logo'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = { 'ag_news': ('18wYKf0IKTu2fWUUZQ_DPonZFfh-RkUkU','ag_news_csv.zip', 'zip'),\n",
    "             'amazon_review_full': ('13quYvDyutGnWSSwhYg4CopAaj4OXxRrR','amazon_review_full_csv.zip', 'zip'),\n",
    "             'amazon_review_polarity': ('1DGJITPcVJYBD13KaA26wl6xhfYQ2Kfol','amazon_review_polarity_csv.zip', 'zip'),\n",
    "             'dbpedia': ('1zRBM3-CCms0GqVqmeuoHA4GaeFYGr2KK','dbpedia_csv.zip', 'zip'),\n",
    "             'sogou_news': ('1ghut0YEGDqHzIpevXCLx4Yi3dqHoZ9lk','sogou_news_csv.zip', 'zip'),\n",
    "             'yahoo_answers': ('1Yxp4Y_3BgK2qX2sT9e7VuSGJYTiMfpLM','yahoo_answers_csv.zip', 'zip'),\n",
    "             'yelp_review_full': ('1YDwclFFYy7vMKHPQzUJjyojp4VGht-xL','yelp_review_full_csv.zip', 'zip'),\n",
    "             'yelp_review_polarity': ('1P3N6EHdZUp1ggpRAJCt6Bd7oDDIkM9l5','yelp_review_polarity_csv.zip', 'zip')\n",
    "           }\n",
    "    \n",
    "#https://drive.google.com/open?id=13quYvDyutGnWSSwhYg4CopAaj4OXxRrR\n",
    "#https://drive.google.com/open?id=1DGJITPcVJYBD13KaA26wl6xhfYQ2Kfol\n",
    "#https://drive.google.com/open?id=1zRBM3-CCms0GqVqmeuoHA4GaeFYGr2KK\n",
    "#https://drive.google.com/open?id=1ghut0YEGDqHzIpevXCLx4Yi3dqHoZ9lk\n",
    "#https://drive.google.com/open?id=1Yxp4Y_3BgK2qX2sT9e7VuSGJYTiMfpLM\n",
    "#https://drive.google.com/open?id=1YDwclFFYy7vMKHPQzUJjyojp4VGht-xL\n",
    "#https://drive.google.com/open?id=1P3N6EHdZUp1ggpRAJCt6Bd7oDDIkM9l5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ef08d79363432fb3221e14073d5073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select dataset to use', options=('ag_news', 'amazon_review_full', 'amazon_review_polarit…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wid = Dropdown(options=list(datasets), description='Select dataset to use')\n",
    "display(wid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing data and downloading if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder = 'data' + os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_folder):\n",
    "    os.mkdir(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/JungWinter/halo.git\n",
      "  Cloning git://github.com/JungWinter/halo.git to /private/var/folders/d1/fj_csd896z9gb0n4vp96vd0w0000gp/T/pip-3kjwsolf-build\n",
      "Requirement already satisfied: backports.shutil_get_terminal_size==1.0.0 in /Users/lm/anaconda3/lib/python3.6/site-packages (from halo==0.0.9)\n",
      "Requirement already satisfied: log_symbols==0.0.11 in /Users/lm/anaconda3/lib/python3.6/site-packages (from halo==0.0.9)\n",
      "Requirement already satisfied: spinners==0.0.19 in /Users/lm/anaconda3/lib/python3.6/site-packages (from halo==0.0.9)\n",
      "Requirement already satisfied: cursor==1.2.0 in /Users/lm/anaconda3/lib/python3.6/site-packages (from halo==0.0.9)\n",
      "Requirement already satisfied: termcolor==1.1.0 in /Users/lm/anaconda3/lib/python3.6/site-packages (from halo==0.0.9)\n",
      "Requirement already satisfied: colorama==0.3.9 in /Users/lm/anaconda3/lib/python3.6/site-packages (from halo==0.0.9)\n",
      "Requirement already satisfied: six==1.11.0 in /Users/lm/anaconda3/lib/python3.6/site-packages (from halo==0.0.9)\n",
      "Requirement already satisfied: enum34==1.1.6 in /Users/lm/anaconda3/lib/python3.6/site-packages (from log_symbols==0.0.11->halo==0.0.9)\n",
      "Installing collected packages: halo\n",
      "  Running setup.py install for halo ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed halo-0.0.9\n",
      "You are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+git://github.com/JungWinter/halo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'python-requests/2.18.4', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbase = wid.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbase ='yelp_review_polarity'#'yahoo_answers' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected dataset is ag_news\n"
     ]
    }
   ],
   "source": [
    "print('The selected dataset is', dbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download import download_file_from_google_drive, HaloNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_folder + dbase+'_csv'):\n",
    "    #os.mkdir(data_folder + selected_dataset)\n",
    "    id, file_name, file_type = datasets[dbase]\n",
    "    download_file_from_google_drive(id, data_folder +os.sep+file_name)\n",
    "    \n",
    "    if file_type == 'tgz':\n",
    "        import tarfile\n",
    "        tar = tarfile.open(data_folder + os.sep+file_name)\n",
    "        tar.extractall(data_folder + os.sep)\n",
    "        \n",
    "    if file_type =='zip':\n",
    "        import zipfile\n",
    "        zip_ref = zipfile.ZipFile(data_folder +os.sep+file_name, 'r')\n",
    "        zip_ref.extractall(data_folder + os.sep)\n",
    "        zip_ref.close()\n",
    "        \n",
    "    ## if file exists, delete it ##\n",
    "    if os.path.isfile(data_folder +os.sep+file_name):\n",
    "        os.remove(data_folder +os.sep+file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HaloNotebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8aaf22f5cbf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhalo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHaloNotebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HaloNotebook'"
     ]
    }
   ],
   "source": [
    "from halo import HaloNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halo.Halo https://github.com/JungWinter/halo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457a4288be864637a26e645e4e4b2a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spinner = HaloNotebook(text='Downloading {}'.format('asdasdas.zip'), spinner='dots12')\n",
    "spinner.start()\n",
    "for _ in range(3):\n",
    "    time.sleep(5)\n",
    "spinner.succeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ Downloading ag_news_csv.zip⠙ Downloading ag_news_csv.zip⠹ Downloading ag_news_csv.zip⠸ Downloading ag_news_csv.zip⠼ Downloading ag_news_csv.zip⠴ Downloading ag_news_csv.zip⠦ Downloading ag_news_csv.zip⠧ Downloading ag_news_csv.zip⠇ Downloading ag_news_csv.zip⠏ Downloading ag_news_csv.zip⠋ Downloading ag_news_csv.zip⠙ Downloading ag_news_csv.zip⠹ Downloading ag_news_csv.zip⠸ Downloading ag_news_csv.zip⠼ Downloading ag_news_csv.zip⠴ Downloading ag_news_csv.zip⠦ Downloading ag_news_csv.zip⠧ Downloading ag_news_csv.zip⠇ Downloading ag_news_csv.zip⠏ Downloading ag_news_csv.zip⠋ Downloading ag_news_csv.zip⠙ Downloading ag_news_csv.zip⠹ Downloading ag_news_csv.zip⠸ Downloading ag_news_csv.zip⠼ Downloading ag_news_csv.zip⠴ Downloading ag_news_csv.zip⠦ Downloading ag_news_csv.zip⠧ Downloading ag_news_csv.zip⠇ Downloading ag_news_csv.zip⠏ Downloading ag_news_csv.zip⠋ Downloading ag_news_csv.zip⠙ Downloading ag_news_csv.zip⠹ Downloading ag_news_csv.zip⠸ Downloading ag_news_csv.zip⠼ Downloading ag_news_csv.zip⠴ Downloading ag_news_csv.zip⠦ Downloading ag_news_csv.zip⠧ Downloading ag_news_csv.zip⠇ Downloading ag_news_csv.zip⠏ Downloading ag_news_csv.zip⠋ Downloading ag_news_csv.zip⠙ Downloading ag_news_csv.zip⠹ Downloading ag_news_csv.zip⠸ Downloading ag_news_csv.zip⠼ Downloading ag_news_csv.zip⠴ Downloading ag_news_csv.zip⠦ Downloading ag_news_csv.zip⠧ Downloading ag_news_csv.zip⠇ Downloading ag_news_csv.zip⠏ Downloading ag_news_csv.zip⠋ Downloading ag_news_csv.zip⠙ Downloading ag_news_csv.zip⠹ Downloading ag_news_csv.zip⠸ Downloading ag_news_csv.zip⠼ Downloading ag_news_csv.zip⠴ Downloading ag_news_csv.zip⠦ Downloading ag_news_csv.zip⠧ Downloading ag_news_csv.zip⠇ Downloading ag_news_csv.zip⠏ Downloading ag_news_csv.zip⠋ Downloading ag_news_csv.zip⠙ Downloading ag_news_csv.zip⠹ Downloading ag_news_csv.zip⠸ Downloading ag_news_csv.zip⠼ Downloading ag_news_csv.zip⠴ Downloading ag_news_csv.zip⠦ Downloading ag_news_csv.zip⠧ Downloading ag_news_csv.zip⠇ Downloading ag_news_csv.zip"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-33ad602a33c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mHalo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Downloading {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspinner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dots'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspinner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mspinner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdownload_file_from_google_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/compact-character-level-deep-classification/download.py\u001b[0m in \u001b[0;36mdownload_file_from_google_drive\u001b[0;34m(id, destination)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'asasas'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGGL_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 141\u001b[0;31m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/halo/halo.py\u001b[0m in \u001b[0;36mhandle_keyboard_interrupt\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m\"\"\"Handle KeyboardInterrupt without try-except statement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_keyboard_interrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "id, file_name, file_type = datasets[dbase]\n",
    "\n",
    "with Halo(text='Downloading {}'.format(file_name), spinner='dots') as spinner:\n",
    "    spinner.start()\n",
    "    download_file_from_google_drive(id, os.path.join(data_folder, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "if not os.path.exists('params'):\n",
    "    os.mkdir('params')\n",
    "\n",
    "\n",
    "path_test = 'data/'+dbase+'_csv/test.csv'\n",
    "path_train = 'data/'+dbase+'_csv/train.csv'\n",
    "\n",
    "epochs=10\n",
    "batch_size = 128\n",
    "\n",
    "#filter_sizes = [3,3,3,3]\n",
    "\n",
    "best_acc=0\n",
    "\n",
    "#cod_type='0'\n",
    "\n",
    "#len_words=128\n",
    "#len_cod_word=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Whether to save model parameters\n",
    "\n",
    "model_output = 'params/Word_Word2vec300_'+dbase+'_output.csv'\n",
    "model_prediction_output = 'params/Word__Word2vec300_'+dbase\n",
    "model_real='params/Word__Word2vec300_'+dbase+'_real'\n",
    "\n",
    "model_keras_model='params/Word__Word2vec300_'+dbase+'_model.json'\n",
    "model_keras_wheights='params/Word__Word2vec300_'+dbase+'_weights.h5'\n",
    "\n",
    "glove_dir='glove.6b'\n",
    "dim_glove=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://drive.google.com/uc?export=download&confirm=s5vl&id=13quYvDyutGnWSSwhYg4CopAaj4OXxRrR:  67%|██████▋   | 1.38k/2.05k [00:00<00:00, 806b/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('https://drive.google.com/uc?export=download', id='13quYvDyutGnWSSwhYg4CopAaj4OXxRrR', 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getinfo() takes exactly 1 argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ca8687ea057f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: getinfo() takes exactly 1 argument (0 given)"
     ]
    }
   ],
   "source": [
    "c.getinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def shuffle_file(path_train, path_train_shuffled):\\n    from os import listdir\\n    from os.path import isfile, join\\n\\n    #onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\\n    exist_file =isfile(path_train_shuffled)\\n\\n    doc=[]\\n    if not exist_file:\\n        with io.open(path_train, encoding='utf-8') as f:\\n            for line in f:\\n                doc.append(line)\\n\\n        p = np.random.permutation(len(doc))\\n\\n        with open(path_train_shuffled,'w', encoding='utf-8') as f:\\n            for idp in p:\\n                f.write (doc[p[idp]])\\n            f.close() \\n    \\n\\n\\nreturn (not exist_file)\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def shuffle_file(path_train, path_train_shuffled):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "\n",
    "    #onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    exist_file =isfile(path_train_shuffled)\n",
    "\n",
    "    doc=[]\n",
    "    if not exist_file:\n",
    "        with io.open(path_train, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                doc.append(line)\n",
    "\n",
    "        p = np.random.permutation(len(doc))\n",
    "\n",
    "        with open(path_train_shuffled,'w', encoding='utf-8') as f:\n",
    "            for idp in p:\n",
    "                f.write (doc[p[idp]])\n",
    "            f.close() \n",
    "    \n",
    "\n",
    "\n",
    "return (not exist_file)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# verifying if already exists a shuffle training data file\\n\\ncreated_shuffled = shuffle_file(path_train, path_train_shuffled)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train_shuffled = 'data/'+dbase+'_csv/train_shuffled.csv'\n",
    "\n",
    "'''# verifying if already exists a shuffle training data file\n",
    "\n",
    "created_shuffled = shuffle_file(path_train, path_train_shuffled)\n",
    "'''\n",
    "\n",
    " \n",
    "\n",
    "#if created_shuffled:\n",
    "#    print('shuffled file generated ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning Documents:  560000\n",
      "Validating Documents:  38000\n",
      "Categories:  2\n"
     ]
    }
   ],
   "source": [
    "### category number, # validating docs, # testing docs\n",
    "with io.open(path_train_shuffled, encoding='utf-8') as f:\n",
    "    cat_output=0\n",
    "    ndocs_t=0\n",
    "    for line in f:\n",
    "        \n",
    "        ndocs_t+=1\n",
    "        \n",
    "        data_yx=line.split(',')\n",
    "            \n",
    "        cat = data_yx[0]\n",
    "        cat=re.sub('\"', '', cat)\n",
    "        cat=int(cat)\n",
    "        \n",
    "        if cat>cat_output:\n",
    "            cat_output=cat\n",
    "\n",
    "with io.open(path_test, encoding='utf-8') as f:\n",
    "    ndocs_v=0\n",
    "    for line in f:\n",
    "        ndocs_v+=1\n",
    "\n",
    "print('Trainning Documents: ',ndocs_t)      \n",
    "print('Validating Documents: ',ndocs_v)  \n",
    "print('Categories: ',cat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "x_evaluate=[]\n",
    "y_evaluate=[]\n",
    "\n",
    "with io.open(path_train_shuffled, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        data_yx=line.split(',')\n",
    "        cat = data_yx[0]\n",
    "        cat=re.sub('\"', '', cat)\n",
    "\n",
    "        result=np.zeros((cat_output))\n",
    "        result[np.array(int(cat))-1]=1\n",
    "            \n",
    "        y_train.append(result)\n",
    "            \n",
    "        txt=\"\"\n",
    "        for col_txt in data_yx[1:]:\n",
    "            txt = txt+\" \"+col_txt \n",
    "            \n",
    "        x_train.append(txt)\n",
    "        \n",
    "with io.open(path_test, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        data_yx=line.split(',')\n",
    "        cat = data_yx[0]\n",
    "        cat=re.sub('\"', '', cat)\n",
    "\n",
    "        result=np.zeros((cat_output))\n",
    "        result[np.array(int(cat))-1]=1\n",
    "            \n",
    "        y_evaluate.append(result)\n",
    "            \n",
    "        txt=\"\"\n",
    "        for col_txt in data_yx[1:]:\n",
    "            txt = txt+\" \"+col_txt \n",
    "            \n",
    "        x_evaluate.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Very bad all the way around. First and only time I will ever go. A bunch of younger people that have no concept of customer service. After ordering our food from a snooty girl  we sat and waited for our food for 25 minutes. While waiting  the employee in the back cooking and the girl preparing the take out and eat in orders were cussing and angry at each other. I heard 4 f bombs and a few b words. Horrible atmosphere. Btw the food was mediocre at best. The food was very greasy and tasteless. Do not recommend going unless your a teenager. Very disappointed.\"\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet:  68\n"
     ]
    }
   ],
   "source": [
    "# generating alphabet ordered by frequency\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "with io.open(path_train_shuffled, encoding='utf-8') as f:\n",
    "    c = Counter()\n",
    "    for line in f:\n",
    "        text=line.lower()\n",
    "        c += Counter(list(text))\n",
    "\n",
    "alpha=sorted(c.items(), key=lambda c: c[1],reverse=True)\n",
    "# ranked alphabet by frequency\n",
    "rank = [item[0] for item in alpha]\n",
    "print('Alphabet: ',len(rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'e', 't', 'a', 'o', 'i', 'n', 's', 'r', 'h', 'd', 'l', 'u', 'c', 'w', 'm', 'y', 'f', 'g', 'p', '.', 'b', 'v', 'k', ',', '\"', '\\\\', \"'\", '!', 'x', '\\n', '1', '-', 'j', '2', 'z', '0', ')', '(', 'q', '?', '5', '$', '3', ':', '/', '4', '9', '&', '6', '8', '7', '*', ';', '%', '+', '=', '_', '#', '~', '@', ']', '[', '^', '`', '|', '}', '{']\n"
     ]
    }
   ],
   "source": [
    "print(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "letters_zhang=string.ascii_lowercase+string.digits+string.punctuation+'\\n'\n",
    "letters_zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(letters_zhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max code size: 69\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding, like Zhang, et al 2015\n",
    "\n",
    "cod_zhang= {}\n",
    "decod_zhang={}\n",
    "max_zhang=0\n",
    "for i, l in enumerate(letters_zhang):\n",
    "    cod_zhang[l]=\"0\"*i+\"1\"\n",
    "    decod_zhang[\"0\"*i+\"1\"]=l\n",
    "    if len(cod_zhang[l])>max_zhang:\n",
    "        max_zhang=len(cod_zhang[l])\n",
    "    #print(l,cod_zhang[l])\n",
    "print('Max code size:',max_zhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max code size: 12\n"
     ]
    }
   ],
   "source": [
    "# End Tag Dense Codes - Brisaboa\n",
    "\n",
    "def etdc2(i):\n",
    "    final = ['10','11']\n",
    "    pref=['00','10']\n",
    "    \n",
    "    pos =[]\n",
    "    ndiv=i\n",
    "    while ndiv!=-1:\n",
    "        pos.append(ndiv%2)\n",
    "        ndiv =(ndiv // 2) -1\n",
    "    \n",
    "    #print(pos)\n",
    "    etdc2_cod=''\n",
    "    for l in range(len(pos)-1,0,-1):\n",
    "        etdc2_cod=etdc2_cod+pref[pos[l]]\n",
    "        \n",
    "    etdc2_cod = etdc2_cod+final[pos[0]]\n",
    "    return etdc2_cod\n",
    "\n",
    "max_etdc2=0\n",
    "cod_etdc2 = {}\n",
    "decod_etdc2={}\n",
    "for i, l in enumerate(rank):\n",
    "    cod_etdc2[l]=etdc2(i)\n",
    "    decod_etdc2[etdc2(i)]=l\n",
    "    if len(cod_etdc2[l])>max_etdc2:\n",
    "        max_etdc2=len(cod_etdc2[l])\n",
    "    #print(l,cod_etdc2[l])\n",
    "print('Max code size:', max_etdc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max code size: 69\n"
     ]
    }
   ],
   "source": [
    "cod_word101= {}\n",
    "decod_word101={}\n",
    "max_word101=0\n",
    "for i, l in enumerate(rank):\n",
    "    cod_word101[l]=\"1\"+\"0\"*i+\"1\"\n",
    "    decod_word101[\"1\"+\"0\"*i+\"1\"]=l\n",
    "    if len(cod_word101[l])>max_word101:\n",
    "        max_word101=len(cod_word101[l])\n",
    "    #print(l,cod_word101[l])\n",
    "print('Max code size:',max_word101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max code size: 68\n"
     ]
    }
   ],
   "source": [
    "cod_word01= {}\n",
    "decod_word01={}\n",
    "max_word01=0\n",
    "for i, l in enumerate(rank):\n",
    "    cod_word01[l]=\"0\"*i+\"1\"\n",
    "    decod_word01[\"0\"*i+\"1\"]=l\n",
    "    if len(cod_word01[l])>max_word01:\n",
    "        max_word01=len(cod_word01[l])\n",
    "    #print(l,cod_word01[l])\n",
    "print('Max code size:',max_word01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(txt,vocab, max_code_size):\n",
    "    for i,l in enumerate(txt):\n",
    "        if l in vocab:\n",
    "            print (vocab[l]+\"-\"*(max_code_size-len(vocab[l])))\n",
    "        else:\n",
    "            print (\"-\"*max_code_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_w(txt,vocab, max_cod_size=256):\n",
    "    \n",
    "    # eliminating consecutive spaces\n",
    "    words = ' '.join(txt.split()).lower().split()\n",
    "    \n",
    "    for i,w in enumerate(words):\n",
    "        cod_w =''\n",
    "        for c in w:\n",
    "            if c in vocab:\n",
    "                cod_w +=vocab[c]\n",
    "            else:\n",
    "                cod_w +=\"1\"+len(vocab)+\"1\"\n",
    "                \n",
    "        print (cod_w[0:max_cod_size]+\"-\"*np.max(max_cod_size-len(cod_w),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000011011000000011011000110000000011000000000000011000000000\n",
      "1000001100000001-------------------------------------------------\n",
      "1000110000001----------------------------------------------------\n",
      "1000110000000011001----------------------------------------------\n",
      "1000110000001100000000001----------------------------------------\n",
      "10001------------------------------------------------------------\n",
      "100000001100000000000001100000110110000001100000000000001101-----\n"
     ]
    }
   ],
   "source": [
    "#encode_word('research is an art and a science'\n",
    "encode_w('research is an art and a science',cod_word101,65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000010100000001010001000000001000000000000010000000001-------\n",
      "00000100000001---------------------------------------------------\n",
      "00010000001------------------------------------------------------\n",
      "0001000000001001-------------------------------------------------\n",
      "0001000000100000000001-------------------------------------------\n",
      "0001-------------------------------------------------------------\n",
      "00000001000000000000010000010100000010000000000000101------------\n"
     ]
    }
   ],
   "source": [
    "#encode_word('research is an art and a science'\n",
    "encode_w('research is an art and a science',cod_word01,65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000000001---------------------------------------------------\n",
      "00001----------------------------------------------------------------\n",
      "0000000000000000001--------------------------------------------------\n",
      "00001----------------------------------------------------------------\n",
      "1--------------------------------------------------------------------\n",
      "000000000000000001---------------------------------------------------\n",
      "001------------------------------------------------------------------\n",
      "00000001-------------------------------------------------------------\n",
      "---------------------------------------------------------------------\n",
      "000000001------------------------------------------------------------\n",
      "0000000000000000001--------------------------------------------------\n",
      "---------------------------------------------------------------------\n",
      "1--------------------------------------------------------------------\n",
      "00000000000001-------------------------------------------------------\n",
      "---------------------------------------------------------------------\n",
      "1--------------------------------------------------------------------\n",
      "000000000000000001---------------------------------------------------\n",
      "00000000000000000001-------------------------------------------------\n",
      "---------------------------------------------------------------------\n",
      "1--------------------------------------------------------------------\n",
      "00000000000001-------------------------------------------------------\n",
      "0001-----------------------------------------------------------------\n",
      "---------------------------------------------------------------------\n",
      "1--------------------------------------------------------------------\n",
      "---------------------------------------------------------------------\n",
      "0000000000000000001--------------------------------------------------\n",
      "001------------------------------------------------------------------\n",
      "000000001------------------------------------------------------------\n",
      "00001----------------------------------------------------------------\n",
      "00000000000001-------------------------------------------------------\n",
      "001------------------------------------------------------------------\n",
      "00001----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "encode('research is an art and a science',cod_zhang,max_zhang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001010------\n",
      "11----------\n",
      "000011------\n",
      "11----------\n",
      "0011--------\n",
      "001010------\n",
      "101011------\n",
      "001011------\n",
      "10----------\n",
      "1011--------\n",
      "000011------\n",
      "10----------\n",
      "0011--------\n",
      "000010------\n",
      "10----------\n",
      "0011--------\n",
      "001010------\n",
      "0010--------\n",
      "10----------\n",
      "0011--------\n",
      "000010------\n",
      "100010------\n",
      "10----------\n",
      "0011--------\n",
      "10----------\n",
      "000011------\n",
      "101011------\n",
      "1011--------\n",
      "11----------\n",
      "000010------\n",
      "101011------\n",
      "11----------\n"
     ]
    }
   ],
   "source": [
    "encode('research is an art and a science',cod_etdc2,max_etdc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_time(start):\n",
    "    end=time.time()\n",
    "    m, s = divmod(end-start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return (\"%d:%02d:%02d\" % (h, m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove={}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    glove[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_char(txt,vocab, max_code_size, max_len=1014):\n",
    "    # Iterate over the loaded data and create a matrix of size max_len x max_cod_size\n",
    "    # Longer are cutted, shorter are padded\n",
    "    # Chars not in the vocab are encoded to a missing value.\n",
    "\n",
    "    # lower_case text\n",
    "    sent_chars = txt.lower().replace(' ', '')\n",
    "    \n",
    "    #limiting to max_len\n",
    "    sent_chars = sent_chars[:max_len]   \n",
    "    \n",
    "        \n",
    "    sent_array = np.zeros((max_len,max_code_size))\n",
    "    \n",
    "    for i, l in enumerate(sent_chars):\n",
    "        if l in vocab:\n",
    "            \n",
    "            for j, c in enumerate(vocab[l]):\n",
    "                sent_array[i,j] = np.float32(c)\n",
    "            \n",
    "    return sent_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'frase.', 'This', 'is', 'another']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(' '.join('This is  a frase. This is     another'.split())).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_word(txt,vocab, max_code_size=256, max_nwords=128,dim_glove=0,glove={}):\n",
    "    # Iterate over the loaded data and create a matrix of size max_len x max_cod_size\n",
    "    # Longer are cutted, shorter are padded\n",
    "    # Chars not in the vocab are encoded to a missing value.\n",
    "\n",
    "    # treat continuous spaces as one, lower_case text\n",
    "    \n",
    "    for x in string.punctuation:\n",
    "        txt = txt.replace(x,' '+x+' ')\n",
    "        \n",
    "    sent_words = ' '.join(txt.split()).lower().split()\n",
    "    \n",
    "    #limiting to max_nwords\n",
    "    sent_words = sent_words[:max_nwords]   \n",
    "    \n",
    "        \n",
    "    sent_array = np.zeros((max_nwords,max_code_size+dim_glove))\n",
    "    \n",
    "    for i, w in enumerate(sent_words):\n",
    "        cod_w =''\n",
    "        for c in w:\n",
    "            if c in vocab:\n",
    "                cod_w +=vocab[c]\n",
    "            else:\n",
    "                cod_w +=\"1\"+'0'*len(vocab)+\"1\"\n",
    "                \n",
    "        cod_w=cod_w[0:max_code_size]\n",
    "        for j,c in enumerate(cod_w):\n",
    "                sent_array[i,j] = np.float32(c)\n",
    "        \n",
    "        \n",
    "        if w in glove:\n",
    "            for ig, emb_glove in enumerate(glove[w]):\n",
    "                sent_array[i,ig+max_code_size] = emb_glove\n",
    "        \n",
    "    return sent_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#z=encode_char('abcd',cod_zhang,max_zhang,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#z=encode_word('aaadfadfadase    the',cod_word01,max_code_size=20,max_nwords=10,dim_glove=dim_glove,glove=glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#e[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return self.it.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@threadsafe_generator\n",
    "def gen_text2char_code(x,y,cat_output,batch_size,max_cod_size,vocab, max_len=1014, shuffle=True):\n",
    "\n",
    "    x_shuffled=x\n",
    "    y_shuffled=y\n",
    "    \n",
    "    # Knuth-Fisher-Yates algorithm //  inplace shuffle\n",
    "    if shuffle == True:\n",
    "        for i in reversed(range(1, len(x_shuffled))):\n",
    "        # pick an element in x[:i+1] with which to exchange x[i]\n",
    "            j = int(random.random() * (i+1))\n",
    "            x_shuffled[i], x_shuffled[j] = x_shuffled[j], x_shuffled[i]\n",
    "            y_shuffled[i], y_shuffled[j] = y_shuffled[j], y_shuffled[i]\n",
    "        \n",
    "    n_start =0\n",
    "    \n",
    "    sent_data_x = []\n",
    "    sent_data_y = []\n",
    "    #for n_start in range(0,len(x),batch_gen):  \n",
    "    while True:\n",
    "        \n",
    "        n_sample =np.min((len(x_shuffled)-n_start,batch_size))\n",
    "        #print(x_shuffled[n_start])\n",
    "        sent_data_x = np.zeros((n_sample, max_len, max_cod_size+dim_glove))\n",
    "        sent_data_y = np.zeros((n_sample, cat_output))\n",
    "\n",
    "        \n",
    "        for sample in range(0,n_sample):\n",
    "            #print(sample)\n",
    "            #print(x_shuffled[n_start+sample])\n",
    "            sent_data_y[sample,:] = y_shuffled[n_start+sample]\n",
    "            sent_data_x[sample,:,:] = encode_char(x_shuffled[n_start+sample], vocab, max_cod_size, max_len,dim_glove,glove)\n",
    "       \n",
    "        n_start +=n_sample\n",
    "        \n",
    "        \n",
    "        yield (sent_data_x,sent_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@threadsafe_generator\n",
    "def gen_text2word_code(x,y,cat_output,batch_size,max_cod_size,vocab, max_nwords=128, shuffle=True,dim_glove=0,glove={}):\n",
    "\n",
    "    x_shuffled=x\n",
    "    y_shuffled=y\n",
    "    \n",
    "    # Knuth-Fisher-Yates algorithm //  inplace shuffle\n",
    "    if shuffle == True:\n",
    "        for i in reversed(range(1, len(x_shuffled))):\n",
    "        # pick an element in x[:i+1] with which to exchange x[i]\n",
    "            j = int(random.random() * (i+1))\n",
    "            x_shuffled[i], x_shuffled[j] = x_shuffled[j], x_shuffled[i]\n",
    "            y_shuffled[i], y_shuffled[j] = y_shuffled[j], y_shuffled[i]\n",
    "        \n",
    "    n_start =0\n",
    "    \n",
    "    sent_data_x = []\n",
    "    sent_data_y = []\n",
    "    #for n_start in range(0,len(x),batch_gen):  \n",
    "    while True:\n",
    "        \n",
    "        n_sample =np.min((len(x_shuffled)-n_start,batch_size))\n",
    "        #print(x_shuffled[n_start])\n",
    "        sent_data_x = np.zeros((n_sample, max_nwords, max_cod_size+dim_glove))\n",
    "        sent_data_y = np.zeros((n_sample, cat_output))\n",
    "\n",
    "        \n",
    "        for sample in range(0,n_sample):\n",
    "            #print(sample)\n",
    "            #print(x_shuffled[n_start+sample])\n",
    "            sent_data_y[sample,:] = y_shuffled[n_start+sample]\n",
    "            sent_data_x[sample,:,:] = encode_word(x_shuffled[n_start+sample], vocab, max_cod_size, max_nwords,dim_glove=dim_glove,glove=glove)\n",
    "       \n",
    "        n_start +=n_sample\n",
    "        \n",
    "        \n",
    "        yield (sent_data_x,sent_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gen = #gen_text2word_code(x_train, y_train,cat_output,3,125,cod_word101,5,shuffle=False,dim_glove=dim_glove,glove=glove)\n",
    "#gen = gen_text2char_code(x_train, y_train,cat_output,batch_size,max_etdc2,cod_etdc2,25,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x,y=gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print([''.join(str(int(i))) for i in x[0][3][:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode_w(x_train[0],cod_word101,125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choosing encoding\n",
    "\n",
    "max_cod_size = 256 #max_zhang #max_etdc2 #\n",
    "vocab = cod_word101#cod_word01 #cod_etdc2 #\n",
    "max_nwords=128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_cod_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 Ti (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "### 1 CNN\n",
    "# -*- coding: utf-8 -*-\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Add, BatchNormalization,LeakyReLU,GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout,Concatenate\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "#Define what the input shape looks like\n",
    "\n",
    "inputs = Input(shape=(max_nwords, max_cod_size+dim_glove,),  dtype='float32')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def deeper(input0):\n",
    "    \n",
    "    half_input = MaxPooling1D(pool_size=3, strides=2,padding='same')(input0)\n",
    "    half_input = BatchNormalization()(half_input)\n",
    "    half_input = LeakyReLU()(half_input)\n",
    "   \n",
    "   \n",
    "    conv2 = Conv1D(filters=256,kernel_size=1,activation='elu',kernel_initializer='RandomNormal',padding='same')(half_input)\n",
    "    conv2 =BatchNormalization()(conv2)\n",
    "    conv2 = LeakyReLU()(conv2)\n",
    "    \n",
    "    conv2 = Conv1D(filters=256,kernel_size=1,activation='elu',kernel_initializer='RandomNormal',padding='same')(conv2)\n",
    "    #conv2 =BatchNormalization()(conv2)\n",
    "\n",
    "        \n",
    "        \n",
    "    l_res = Add()([half_input,conv2])\n",
    "    #l_res = LeakyReLU()(l_res)    \n",
    "    \n",
    "    return l_res\n",
    "\n",
    "\n",
    "###### first input ######\n",
    "\n",
    "first_input = Conv1D(filters=256,kernel_size=1,activation='elu',kernel_initializer='RandomNormal',padding='same')(inputs)\n",
    "first_input = BatchNormalization()(first_input)\n",
    "first_input = LeakyReLU()(first_input)\n",
    "   \n",
    "\n",
    "\n",
    "conv0 = Conv1D(filters=256,kernel_size=1,activation='elu',kernel_initializer='RandomNormal',padding='same')(first_input)\n",
    "conv0 = BatchNormalization()(conv0)\n",
    "conv0 = LeakyReLU()(conv0)\n",
    "    \n",
    "conv0 = Conv1D(filters=256,kernel_size=1,activation='elu',kernel_initializer='RandomNormal',padding='same')(conv0)\n",
    "\n",
    "l_res0 = Add()([first_input,conv0])\n",
    "\n",
    "\n",
    "##### deep resnet ######\n",
    "\n",
    "\n",
    "l_out = (deeper(deeper(deeper(deeper(deeper(deeper(deeper(l_res0))))))))\n",
    "    \n",
    "    \n",
    "l_out = GlobalMaxPooling1D()(l_out)\n",
    "\n",
    "\n",
    "pred = Dense(cat_output, activation='softmax')(l_out)\n",
    " \n",
    "model = Model(inputs=inputs, outputs=pred)\n",
    "\n",
    "# Reloading Saved params\n",
    "#model.load_weights(model_keras_wheights[:-3]+'-last.h5', by_name=True)\n",
    "#print ('weights loaded...')\n",
    "    \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 128, 556)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 128, 256)      142592      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 128, 256)      1024        conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)        (None, 128, 256)      0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 128, 256)      65792       leaky_re_lu_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 128, 256)      1024        conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)        (None, 128, 256)      0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 128, 256)      65792       leaky_re_lu_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 128, 256)      0           leaky_re_lu_1[0][0]              \n",
      "                                                                   conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 64, 256)       0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 64, 256)       1024        max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)        (None, 64, 256)       0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 64, 256)       65792       leaky_re_lu_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 64, 256)       1024        conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)        (None, 64, 256)       0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 64, 256)       65792       leaky_re_lu_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 64, 256)       0           leaky_re_lu_3[0][0]              \n",
      "                                                                   conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 32, 256)       0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 32, 256)       1024        max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)        (None, 32, 256)       0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 32, 256)       65792       leaky_re_lu_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 32, 256)       1024        conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)        (None, 32, 256)       0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 32, 256)       65792       leaky_re_lu_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 32, 256)       0           leaky_re_lu_5[0][0]              \n",
      "                                                                   conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 16, 256)       0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 16, 256)       1024        max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)        (None, 16, 256)       0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 16, 256)       65792       leaky_re_lu_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 16, 256)       1024        conv1d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)        (None, 16, 256)       0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 16, 256)       65792       leaky_re_lu_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 16, 256)       0           leaky_re_lu_7[0][0]              \n",
      "                                                                   conv1d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 8, 256)        0           add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 8, 256)        1024        max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)        (None, 8, 256)        0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)               (None, 8, 256)        65792       leaky_re_lu_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 8, 256)        1024        conv1d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)       (None, 8, 256)        0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)               (None, 8, 256)        65792       leaky_re_lu_10[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 8, 256)        0           leaky_re_lu_9[0][0]              \n",
      "                                                                   conv1d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 4, 256)        0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 4, 256)        1024        max_pooling1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)       (None, 4, 256)        0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)               (None, 4, 256)        65792       leaky_re_lu_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 4, 256)        1024        conv1d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)       (None, 4, 256)        0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)               (None, 4, 256)        65792       leaky_re_lu_12[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 4, 256)        0           leaky_re_lu_11[0][0]             \n",
      "                                                                   conv1d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)   (None, 2, 256)        0           add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 2, 256)        1024        max_pooling1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)       (None, 2, 256)        0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)               (None, 2, 256)        65792       leaky_re_lu_13[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 2, 256)        1024        conv1d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)       (None, 2, 256)        0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)               (None, 2, 256)        65792       leaky_re_lu_14[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 2, 256)        0           leaky_re_lu_13[0][0]             \n",
      "                                                                   conv1d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)   (None, 1, 256)        0           add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_15 (BatchNor (None, 1, 256)        1024        max_pooling1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)       (None, 1, 256)        0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)               (None, 1, 256)        65792       leaky_re_lu_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 1, 256)        1024        conv1d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)       (None, 1, 256)        0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)               (None, 1, 256)        65792       leaky_re_lu_16[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 1, 256)        0           leaky_re_lu_15[0][0]             \n",
      "                                                                   conv1d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalMa (None, 256)           0           add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             514         global_max_pooling1d_1[0][0]     \n",
      "====================================================================================================\n",
      "Total params: 1,212,162\n",
      "Trainable params: 1,203,970\n",
      "Non-trainable params: 8,192\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "logger = CSVLogger(model_output, separator=',', append=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def evaluating(epoch,path_test,cat_output,batch_gen,len_words,len_cod_word)\n",
    "def evaluating(epoch, x_evaluate,y_evaluate,cat_output,batch_size,max_cod_size,vocab, max_nwords, shuffle=False,dim_glove=0,glove={}):\n",
    "\n",
    "\n",
    "    ev_batch = gen_text2word_code(x_evaluate, y_evaluate,cat_output,batch_size,\n",
    "                                  max_cod_size,vocab, max_nwords, shuffle,dim_glove=dim_glove,glove=glove)\n",
    "\n",
    "    pred=list()\n",
    "    real=list()\n",
    "    nbatchs_test = ceil(len(x_evaluate)/batch_size)\n",
    "    for ibatch in range(nbatchs_test):\n",
    "        x_ev,y_ev = ev_batch.__next__()\n",
    "        prediction = model.predict(x_ev)\n",
    "        pred=pred+list(prediction)\n",
    "        real = real +list(y_ev)\n",
    "        \n",
    "    r=np.argmax(real,axis=1)\n",
    "    p=np.argmax(pred,axis=1)\n",
    "    \n",
    "    acc = np.sum(r==p)/len(r)\n",
    "    \n",
    "    np.savetxt(model_real+'_epoch_'+str(e)+'.txt', real,fmt='%1i')\n",
    "    np.savetxt(model_prediction_output+'_epoch_'+str(e)+'_acc_'+'{:06.4f}'.format(acc)+'.txt', pred,fmt='%06.4f')\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0\n",
      "\n",
      "Epoch 1/1\n",
      "4374/4374 [==============================] - 3496s - loss: 0.2224 - acc: 0.9076  \n",
      "\n",
      "Epoch 0: 0.9238\n",
      "Time Elapsed:  Total: 1:02:59 Epoch:1:02:59\n",
      "Saved model to disk\n",
      "\n",
      "epoch: 1\n",
      "\n",
      "Epoch 1/1\n",
      "4374/4374 [==============================] - 3484s - loss: 0.1755 - acc: 0.9290  \n",
      "\n",
      "Epoch 1: 0.9324\n",
      "Time Elapsed:  Total: 2:05:08 Epoch:1:02:08\n",
      "Saved model to disk\n",
      "\n",
      "epoch: 2\n",
      "\n",
      "Epoch 1/1\n",
      "4374/4374 [==============================] - 3476s - loss: 0.1563 - acc: 0.9373  \n",
      "\n",
      "Epoch 2: 0.9379\n",
      "Time Elapsed:  Total: 3:07:09 Epoch:1:02:00\n",
      "Saved model to disk\n",
      "\n",
      "epoch: 3\n",
      "\n",
      "Epoch 1/1\n",
      "4374/4374 [==============================] - 3528s - loss: 0.1406 - acc: 0.9440  \n",
      "\n",
      "Epoch 3: 0.9385\n",
      "Time Elapsed:  Total: 4:09:59 Epoch:1:02:50\n",
      "Saved model to disk\n",
      "\n",
      "epoch: 4\n",
      "\n",
      "Epoch 1/1\n",
      "4374/4374 [==============================] - 3501s - loss: 0.1273 - acc: 0.9494  \n",
      "\n",
      "Epoch 4: 0.9401\n",
      "Time Elapsed:  Total: 5:12:22 Epoch:1:02:22\n",
      "Saved model to disk\n",
      "\n",
      "epoch: 5\n",
      "\n",
      "Epoch 1/1\n",
      "4374/4374 [==============================] - 3496s - loss: 0.1138 - acc: 0.9556  \n",
      "\n",
      "Epoch 5: 0.9417\n",
      "Time Elapsed:  Total: 6:14:40 Epoch:1:02:18\n",
      "Saved model to disk\n",
      "\n",
      "epoch: 6\n",
      "\n",
      "Epoch 1/1\n",
      "4374/4374 [==============================] - 3502s - loss: 0.1015 - acc: 0.9605  \n",
      "\n",
      "Epoch 6: 0.9399\n",
      "Time Elapsed:  Total: 7:17:10 Epoch:1:02:29\n",
      "\n",
      "epoch: 7\n",
      "\n",
      "Epoch 1/1\n",
      "4374/4374 [==============================] - 3491s - loss: 0.0888 - acc: 0.9658  \n",
      "\n",
      "Epoch 7: 0.9398\n",
      "Time Elapsed:  Total: 8:19:25 Epoch:1:02:14\n",
      "\n",
      "epoch: 8\n",
      "\n",
      "Epoch 1/1\n",
      "4374/4374 [==============================] - 3549s - loss: 0.0773 - acc: 0.9703  \n",
      "\n",
      "Epoch 8: 0.9393\n",
      "Time Elapsed:  Total: 9:22:45 Epoch:1:03:20\n",
      "\n",
      "epoch: 9\n",
      "\n",
      "Epoch 1/1\n",
      " 226/4374 [>.............................] - ETA: 3397s - loss: 0.0530 - acc: 0.9817"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-dd4ee8141418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     model.fit_generator(generator=gen,steps_per_epoch=nbatchs,\n\u001b[1;32m     29\u001b[0m                         \u001b[0;31m#validation_data=ev_batch, validation_steps=nbatchs_test,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                         epochs=1,verbose=1,max_q_size=100, workers=1, callbacks=[logger])\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/keras2/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/keras2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1869\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1871\u001b[0;31m                             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "total_start = time.time()\n",
    "\n",
    "for e in range(0,epochs):\n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    trained= 0\n",
    "    print ()\n",
    "    print ('epoch:',e)\n",
    "    print ()\n",
    "    \n",
    "    gen=gen_text2word_code(x_train,y_train,cat_output,batch_size,\n",
    "                           max_cod_size,vocab, max_nwords, shuffle=True,dim_glove=dim_glove,glove=glove)\n",
    "    nbatchs = ceil(len(x_train)/batch_size)-1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ev_batch = gen_text2word_code(x_evaluate, y_evaluate,cat_output,batch_size,\n",
    "                                  max_cod_size,vocab, max_nwords, shuffle=False,dim_glove=dim_glove,glove=glove)\n",
    "    nbatchs_test = ceil(len(x_evaluate)/batch_size)\n",
    "   \n",
    "    \n",
    "    \n",
    "    batch_start = time.time()\n",
    "    \n",
    "    model.fit_generator(generator=gen,steps_per_epoch=nbatchs,\n",
    "                        #validation_data=ev_batch, validation_steps=nbatchs_test,\n",
    "                        epochs=1,verbose=1,max_q_size=100, workers=1, callbacks=[logger])\n",
    "    \n",
    "     \n",
    "    batch_end = time.time()\n",
    "        \n",
    "    print ()\n",
    "    \n",
    "    #evaluating \n",
    "    acc= evaluating(e,x_evaluate,y_evaluate,cat_output,batch_size,\n",
    "                    max_cod_size,vocab, max_nwords, shuffle=False,dim_glove=dim_glove,glove=glove)\n",
    "\n",
    "    \n",
    "    print ('Epoch '+str(e)+': {:06.4f}'.format(acc))\n",
    "    print('Time Elapsed:  Total: '+show_time(total_start)+' Epoch:'+show_time(epoch_start))\n",
    "    \n",
    "    if acc>best_acc:\n",
    "        best_acc=acc\n",
    "\n",
    "        # serialize model to JSON\n",
    "        with open(model_keras_model, \"w\") as json_file:\n",
    "            json_file.write(model.to_json())\n",
    "            \n",
    "        # serialize best weights to HDF5\n",
    "        model.save_weights(model_keras_wheights)\n",
    "        print(\"Saved model to disk\")\n",
    "    \n",
    "    model.save_weights(model_keras_wheights[:-3]+'-last.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "65*128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
